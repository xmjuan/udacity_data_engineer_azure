{"cells":[{"cell_type":"markdown","source":["# Reading and Writing Data with Spark\n\nThis notebook contains code that is similar to an exercise from a previous lesson. The differences here are related to running this in Databricks where you do not need to create your own spark sessions as this is handled by the cluster.\n\nRun the code cells to see how everything works. \n\nFirst, let's see what the sparkContext from our cluster looks like and how spark is configured for us:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ed2d2a-657c-4722-ade3-40721cc4fad3"}}},{"cell_type":"code","source":["spark.sparkContext.getConf().getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d3b1598-3f51-49cb-9c7c-dd4fb5a69be9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[23]: [(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;1&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/databricks-hive/*&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;dbfs:/user/hive/warehouse&#39;),\n (&#39;spark.databricks.managedCatalog.clientClassName&#39;,\n  &#39;com.databricks.managedcatalog.ManagedCatalogClientImpl&#39;),\n (&#39;spark.hadoop.fs.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.limit&#39;, &#39;20&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.139.0.4&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.139.64.4&#39;),\n (&#39;spark.databricks.clusterUsageTags.effectiveSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException&#39;,\n  &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.driver.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED&#39;),\n (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.interval&#39;, &#39;2000&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.azure.authorization.caching.enable&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;6463660330994956308&#39;),\n (&#39;spark.master&#39;, &#39;spark://10.139.64.4:7077&#39;),\n (&#39;spark.databricks.sql.configMapperClass&#39;,\n  &#39;com.databricks.dbsql.config.SqlConfigMapperBridge&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.legacy.createHiveTableByDefault&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.attempts.maximum&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0420-014945-374ax7z1&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.throttle.interval&#39;, &#39;500ms&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.databricks.wsfsPublicPreview&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMinWorkers&#39;, &#39;2&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.attempts&#39;, &#39;10&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-6463660330994956308-776cc3bf-f0bd-4f3e-9179-4ad21a2822c0&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.sql.sources.default&#39;, &#39;delta&#39;),\n (&#39;spark.hadoop.fs.mcfs-gs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.app.startTime&#39;, &#39;1650419654001&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.passthrough.oauth.refresher.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.workspaceUrl&#39;,\n  &#39;adb-8332887818640824.4.azuredatabricks.net&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.hadoop.fs.elfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.hadoop.parquet.abfs.readahead.optimization.enabled&#39;, &#39;false&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMaxWorkers&#39;, &#39;3&#39;),\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.hadoop.fs.dbfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFS&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;libraryDownload.sleepIntervalSeconds&#39;, &#39;5&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;student_7pmnz7d1kzair1ax_00725930@vocareumvocareum.onmicrosoft.com&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;democluster&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;0420-014945-374ax7z1&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8332887818640824&#34;}]&#39;),\n (&#39;spark.databricks.managedCatalog.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider&#39;),\n (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;15&#39;),\n (&#39;spark.databricks.clusterUsageTags.managedResourceGroup&#39;,\n  &#39;databricks-rg-demoworkspace-uirmzuikmhlr4&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;workerenv-8332887818640824&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled&#39;,\n  &#39;true&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape&#39;, &#39;false&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;libraryDownload.timeoutSeconds&#39;, &#39;180&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.clusterUsageTags.azureSubscriptionId&#39;,\n  &#39;94ec3a64-dcfe-4219-9e29-88221690c382&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.block.size&#39;, &#39;67108864&#39;),\n (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.ui.port&#39;, &#39;44338&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_budget&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;azure_disk_volume_type: PREMIUM_LRS\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.file.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;westus&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.elfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND_AZURE&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_suite&#39;, &#39;&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;1&#39;),\n (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.databricks.clusterUsageTags.ngrokNpipEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.metadata.validation.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;true&#39;),\n (&#39;spark.databricks.acl.scim.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;8344960321433847&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3a.assumed.role.credentials.provider&#39;,\n  &#39;com.amazonaws.auth.InstanceProfileCredentialsProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.executor.memory&#39;, &#39;3157m&#39;),\n (&#39;spark.databricks.cloudfetch.hasRegionSupport&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n  &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.139.64.4&#39;),\n (&#39;spark.sparklyr-backend.threads&#39;, &#39;1&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice&#39;, &#39;-1.0&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.hadoop.fs.idbfs.impl&#39;, &#39;com.databricks.io.idbfs.IdbfsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;8332887818640824&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterUnityCatalogMode&#39;, &#39;CUSTOM&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.delta.sharing.profile.provider.class&#39;,\n  &#39;io.delta.sharing.DeltaSharingCredentialsProvider&#39;),\n (&#39;spark.databricks.managedCatalog.gcs.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider&#39;),\n (&#39;spark.worker.aioaLazyConfig.iamReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosIamRoleCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;autoscaling&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;workerenv-8332887818640824&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_service&#39;, &#39;&#39;),\n (&#39;spark.databricks.metrics.filesystem_io_metrics&#39;, &#39;true&#39;),\n (&#39;spark.databricks.cloudfetch.requesterClassName&#39;,\n  &#39;com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.2_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_2--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_2--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.5.0-4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.gson--gson--com.google.code.gson__gson__2.8.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.istack--istack-commons-runtime--com.sun.istack__istack-commons-runtime__3.0.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill-java--com.twitter__chill-java__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill_2.12--com.twitter__chill_2.12__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-codec--commons-codec--commons-codec__commons-codec__1.15.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-io--commons-io--commons-io__commons-io__2.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--blas--dev.ludovic.netlib__blas__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--lapack--dev.ludovic.netlib__lapack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.airlift--aircompressor--io.airlift__aircompressor__0.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.4.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.netty--netty-all--io.netty__netty-all__4.1.68.Final.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.servlet--jakarta.servlet-api--jakarta.servlet__jakarta.servlet-api__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.3.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--joda-time--joda-time--joda-time__joda-time__2.10.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--o\n*** WARNING: skipped 86485 bytes of output ***\n\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.139.64.4:43533/classes&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSizeType&#39;, &#39;VM_CONTAINER&#39;),\n (&#39;spark.hadoop.databricks.fs.perfMetrics.enable&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;931b260a3f104de1bc3034bc164cb253&#39;),\n (&#39;spark.hadoop.fs.gs.outputstream.upload.chunk.size&#39;, &#39;16777216&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.databricks.clusterUsageTags.privateLinkEnabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.aws.secretKey.redactor&#39;,\n  &#39;com.databricks.spark.util.AWSSecretKeyRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumCustomTags&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/Rtmp2Unrs7&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n (&#39;spark.databricks.secret.sparkConf.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env&#39;, &#39;&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n (&#39;spark.app.id&#39;, &#39;app-20220420015418-0000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.max.total.tasks&#39;, &#39;1000&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;104.42.101.153&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.MultiClusterLogStore&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;democluster&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;westus&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;,\n  &#39;fa8156c19b3f40be98a67631de656e7f&#39;),\n (&#39;spark.hadoop.parquet.filter.columnindex.enabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.driver.port&#39;, &#39;43533&#39;),\n (&#39;spark.hadoop.fs.dbfsartifacts.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFSV1&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.timeout&#39;, &#39;50000&#39;),\n (&#39;spark.databricks.secret.envVar.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: [(&#39;spark.databricks.preemption.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterFirstOnDemand&#39;, &#39;1&#39;),\n (&#39;spark.sql.hive.metastore.jars&#39;, &#39;/databricks/databricks-hive/*&#39;),\n (&#39;spark.driver.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.sql.warehouse.dir&#39;, &#39;dbfs:/user/hive/warehouse&#39;),\n (&#39;spark.databricks.managedCatalog.clientClassName&#39;,\n  &#39;com.databricks.managedcatalog.ManagedCatalogClientImpl&#39;),\n (&#39;spark.hadoop.fs.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem&#39;),\n (&#39;spark.executor.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.limit&#39;, &#39;20&#39;),\n (&#39;spark.sql.streaming.checkpointFileManagerClass&#39;,\n  &#39;com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager&#39;),\n (&#39;spark.databricks.service.dbutils.repl.backend&#39;,\n  &#39;com.databricks.dbconnect.ReplDBUtils&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstancePrivateIp&#39;, &#39;10.139.0.4&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.allowBatching&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterSource&#39;, &#39;UI&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkVersion&#39;, &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.hive.server2.transport.mode&#39;, &#39;http&#39;),\n (&#39;spark.driver.host&#39;, &#39;10.139.64.4&#39;),\n (&#39;spark.databricks.clusterUsageTags.effectiveSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException&#39;,\n  &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.hailEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.containerType&#39;, &#39;LXC&#39;),\n (&#39;spark.eventLog.enabled&#39;, &#39;false&#39;),\n (&#39;spark.driver.extraJavaOptions&#39;,\n  &#39;-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED&#39;),\n (&#39;spark.databricks.clusterUsageTags.isIMv2Enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.interval&#39;, &#39;2000&#39;),\n (&#39;spark.executor.tempDirectory&#39;, &#39;/local_disk0/tmp&#39;),\n (&#39;spark.hadoop.fs.azure.authorization.caching.enable&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.mapred.output.committer.class&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DirectOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterTargetWorkers&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version&#39;, &#39;2&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.port&#39;, &#39;10000&#39;),\n (&#39;spark.sql.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.home&#39;, &#39;/databricks/spark&#39;),\n (&#39;spark.hadoop.hive.server2.idle.operation.timeout&#39;, &#39;7200000&#39;),\n (&#39;spark.task.reaper.enabled&#39;, &#39;true&#39;),\n (&#39;spark.storage.memoryFraction&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.sparkContextId&#39;, &#39;6463660330994956308&#39;),\n (&#39;spark.master&#39;, &#39;spark://10.139.64.4:7077&#39;),\n (&#39;spark.databricks.sql.configMapperClass&#39;,\n  &#39;com.databricks.dbsql.config.SqlConfigMapperBridge&#39;),\n (&#39;spark.driver.maxResultSize&#39;, &#39;4g&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.delta.multiClusterWrites.enabled&#39;, &#39;true&#39;),\n (&#39;spark.worker.cleanup.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.legacy.createHiveTableByDefault&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.matplotlibInline.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.attempts.maximum&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableCredentialPassthrough&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJdbcAutoStart&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterId&#39;, &#39;0420-014945-374ax7z1&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.retry.throttle.interval&#39;, &#39;500ms&#39;),\n (&#39;spark.hadoop.fs.wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterLogDestination&#39;, &#39;&#39;),\n (&#39;spark.databricks.wsfsPublicPreview&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMinWorkers&#39;, &#39;2&#39;),\n (&#39;spark.cleaner.referenceTracking.blocking&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.isSingleUserCluster&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterState&#39;, &#39;Pending&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.azure.class&#39;,\n  &#39;com.databricks.tahoe.store.AzureLogStore&#39;),\n (&#39;spark.hadoop.fs.azure.skip.metrics&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.hive.hmshandler.retry.attempts&#39;, &#39;10&#39;),\n (&#39;spark.repl.class.outputDir&#39;,\n  &#39;/local_disk0/tmp/repl/spark-6463660330994956308-776cc3bf-f0bd-4f3e-9179-4ad21a2822c0&#39;),\n (&#39;spark.scheduler.mode&#39;, &#39;FAIR&#39;),\n (&#39;spark.sql.sources.default&#39;, &#39;delta&#39;),\n (&#39;spark.hadoop.fs.mcfs-gs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.cpfs-adl.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.app.startTime&#39;, &#39;1650419654001&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3n.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.passthrough.oauth.refresher.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient&#39;),\n (&#39;spark.databricks.workerNodeTypeId&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.sql.hive.metastore.sharedPrefixes&#39;,\n  &#39;org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks&#39;),\n (&#39;spark.databricks.io.directoryCommit.enableLogicalDelete&#39;, &#39;false&#39;),\n (&#39;spark.task.reaper.killTimeout&#39;, &#39;60s&#39;),\n (&#39;spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.min&#39;, &#39;10&#39;),\n (&#39;spark.hadoop.hive.server2.use.SSL&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.databricks.workspaceUrl&#39;,\n  &#39;adb-8332887818640824.4.azuredatabricks.net&#39;),\n (&#39;spark.hadoop.databricks.dbfs.client.version&#39;, &#39;v2&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNodeType&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.path&#39;,\n  &#39;/databricks/keys/jetty-ssl-driver-keystore.jks&#39;),\n (&#39;spark.hadoop.fs.elfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.redactor&#39;,\n  &#39;com.databricks.logging.secrets.CredentialRedactorProxyImpl&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPinned&#39;, &#39;false&#39;),\n (&#39;spark.databricks.acl.provider&#39;,\n  &#39;com.databricks.sql.acl.ReflectionBackedAclProvider&#39;),\n (&#39;spark.hadoop.parquet.abfs.readahead.optimization.enabled&#39;, &#39;false&#39;),\n (&#39;spark.extraListeners&#39;,\n  &#39;com.databricks.backend.daemon.driver.DBCEventLoggingListener&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMaxWorkers&#39;, &#39;3&#39;),\n (&#39;spark.sql.parquet.cacheMetadata&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.adl.impl&#39;, &#39;com.databricks.adl.AdlFileSystem&#39;),\n (&#39;spark.hadoop.fs.dbfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFS&#39;),\n (&#39;spark.hadoop.fs.cpfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableLocalDiskEncryption&#39;, &#39;false&#39;),\n (&#39;spark.databricks.tahoe.logStore.class&#39;,\n  &#39;com.databricks.tahoe.store.DelegatingLogStore&#39;),\n (&#39;libraryDownload.sleepIntervalSeconds&#39;, &#39;5&#39;),\n (&#39;spark.sql.hive.convertMetastoreParquet&#39;, &#39;true&#39;),\n (&#39;spark.databricks.service.dbutils.server.backend&#39;,\n  &#39;com.databricks.dbconnect.SparkServerDBUtils&#39;),\n (&#39;spark.executor.id&#39;, &#39;driver&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAllTags&#39;,\n  &#39;[{&#34;key&#34;:&#34;Vendor&#34;,&#34;value&#34;:&#34;Databricks&#34;},{&#34;key&#34;:&#34;Creator&#34;,&#34;value&#34;:&#34;student_7pmnz7d1kzair1ax_00725930@vocareumvocareum.onmicrosoft.com&#34;},{&#34;key&#34;:&#34;ClusterName&#34;,&#34;value&#34;:&#34;democluster&#34;},{&#34;key&#34;:&#34;ClusterId&#34;,&#34;value&#34;:&#34;0420-014945-374ax7z1&#34;},{&#34;key&#34;:&#34;DatabricksEnvironment&#34;,&#34;value&#34;:&#34;workerenv-8332887818640824&#34;}]&#39;),\n (&#39;spark.databricks.managedCatalog.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider&#39;),\n (&#39;spark.databricks.repl.enableClassFileCleanup&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.size&#39;, &#39;10485760&#39;),\n (&#39;spark.metrics.conf&#39;, &#39;/databricks/spark/conf/metrics.properties&#39;),\n (&#39;spark.databricks.clusterUsageTags.autoTerminationMinutes&#39;, &#39;15&#39;),\n (&#39;spark.databricks.clusterUsageTags.managedResourceGroup&#39;,\n  &#39;databricks-rg-demoworkspace-uirmzuikmhlr4&#39;),\n (&#39;spark.akka.frameSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverNodeType&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvId&#39;,\n  &#39;workerenv-8332887818640824&#39;),\n (&#39;spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled&#39;,\n  &#39;true&#39;),\n (&#39;spark.sql.streaming.stopTimeout&#39;, &#39;15s&#39;),\n (&#39;spark.hadoop.hive.server2.keystore.password&#39;, &#39;[REDACTED]&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape&#39;, &#39;false&#39;),\n (&#39;spark.databricks.overrideDefaultCommitProtocol&#39;,\n  &#39;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&#39;),\n (&#39;spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNoDriverDaemon&#39;, &#39;false&#39;),\n (&#39;libraryDownload.timeoutSeconds&#39;, &#39;180&#39;),\n (&#39;spark.hadoop.parquet.memory.pool.ratio&#39;, &#39;0.5&#39;),\n (&#39;spark.databricks.clusterUsageTags.azureSubscriptionId&#39;,\n  &#39;94ec3a64-dcfe-4219-9e29-88221690c382&#39;),\n (&#39;spark.databricks.passthrough.adls.gen2.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.block.size&#39;, &#39;67108864&#39;),\n (&#39;spark.databricks.tahoe.logStore.gcp.class&#39;,\n  &#39;com.databricks.tahoe.store.GCPLogStore&#39;),\n (&#39;spark.serializer.objectStreamReset&#39;, &#39;100&#39;),\n (&#39;spark.sql.sources.commitProtocolClass&#39;,\n  &#39;com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol&#39;),\n (&#39;spark.hadoop.fs.abfss.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem&#39;),\n (&#39;spark.ui.port&#39;, &#39;44338&#39;),\n (&#39;spark.hadoop.fs.fcfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_budget&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType&#39;,\n  &#39;azure_disk_volume_type: PREMIUM_LRS\\n&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterPythonVersion&#39;, &#39;3&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableDfAcls&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount&#39;, &#39;0&#39;),\n (&#39;spark.shuffle.service.enabled&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.file.impl&#39;,\n  &#39;com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-s3n.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.region&#39;, &#39;westus&#39;),\n (&#39;spark.hadoop.fs.s3a.multipart.threshold&#39;, &#39;104857600&#39;),\n (&#39;spark.rpc.message.maxSize&#39;, &#39;256&#39;),\n (&#39;spark.hadoop.fs.elfs.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterAvailability&#39;, &#39;ON_DEMAND_AZURE&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_suite&#39;, &#39;&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterWorkers&#39;, &#39;1&#39;),\n (&#39;spark.databricks.driverNfs.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterMetastoreAccessType&#39;,\n  &#39;RDS_DIRECT&#39;),\n (&#39;spark.databricks.clusterUsageTags.ngrokNpipEnabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.parquet.page.metadata.validation.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.glue.executorServiceFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory&#39;),\n (&#39;spark.hadoop.fs.abfs.impl&#39;,\n  &#39;shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableElasticDisk&#39;, &#39;true&#39;),\n (&#39;spark.databricks.acl.scim.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.DriverToWebappScimClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.adl.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.block.size.row.check.max&#39;, &#39;10&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerUserId&#39;, &#39;8344960321433847&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.maximum&#39;, &#39;200&#39;),\n (&#39;spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.s3a.assumed.role.credentials.provider&#39;,\n  &#39;com.amazonaws.auth.InstanceProfileCredentialsProvider&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.active.blocks&#39;, &#39;32&#39;),\n (&#39;spark.shuffle.reduceLocality.enabled&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.sql.sources.outputCommitterClass&#39;,\n  &#39;com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled&#39;, &#39;false&#39;),\n (&#39;spark.sql.parquet.compression.codec&#39;, &#39;snappy&#39;),\n (&#39;spark.databricks.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.executor.memory&#39;, &#39;3157m&#39;),\n (&#39;spark.databricks.cloudfetch.hasRegionSupport&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories&#39;,\n  &#39;false&#39;),\n (&#39;spark.hadoop.fs.wasb.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.glue.credentialsProviderFactoryClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerPrivateIp&#39;, &#39;10.139.64.4&#39;),\n (&#39;spark.sparklyr-backend.threads&#39;, &#39;1&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice&#39;, &#39;-1.0&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasb.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.FixedCredentialsFileSystem&#39;),\n (&#39;spark.databricks.passthrough.s3a.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider&#39;),\n (&#39;spark.databricks.session.share&#39;, &#39;false&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterResourceClass&#39;, &#39;default&#39;),\n (&#39;spark.hadoop.fs.idbfs.impl&#39;, &#39;com.databricks.io.idbfs.IdbfsFileSystem&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterOwnerOrgId&#39;, &#39;8332887818640824&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSku&#39;, &#39;STANDARD_SKU&#39;),\n (&#39;spark.hadoop.fs.gs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterUnityCatalogMode&#39;, &#39;CUSTOM&#39;),\n (&#39;spark.databricks.driverNodeTypeId&#39;, &#39;Standard_F4&#39;),\n (&#39;spark.delta.sharing.profile.provider.class&#39;,\n  &#39;io.delta.sharing.DeltaSharingCredentialsProvider&#39;),\n (&#39;spark.databricks.managedCatalog.gcs.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider&#39;),\n (&#39;spark.worker.aioaLazyConfig.iamReadinessCheckClientClass&#39;,\n  &#39;com.databricks.backend.daemon.driver.NephosIamRoleCheckClient&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterScalingType&#39;, &#39;autoscaling&#39;),\n (&#39;spark.databricks.clusterUsageTags.workerEnvironmentId&#39;,\n  &#39;workerenv-8332887818640824&#39;),\n (&#39;spark.hadoop.parquet.page.size.check.estimate&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.spark.driverproxy.customHeadersToProperties&#39;,\n  &#39;X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name&#39;),\n (&#39;spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class&#39;,\n  &#39;com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_service&#39;, &#39;&#39;),\n (&#39;spark.databricks.metrics.filesystem_io_metrics&#39;, &#39;true&#39;),\n (&#39;spark.databricks.cloudfetch.requesterClassName&#39;,\n  &#39;com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester&#39;),\n (&#39;spark.databricks.delta.logStore.crossCloud.fatal&#39;, &#39;true&#39;),\n (&#39;spark.files.fetchFailure.unRegisterOutputOnHost&#39;, &#39;true&#39;),\n (&#39;spark.executor.extraClassPath&#39;,\n  &#39;/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/----com_google_protobuf--timestamp_proto-spark_3.2_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-hive2-client_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark3.2-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:/databricks/jars/----workspace_spark_3_2--common--kvstore--kvstore-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-common--network-common-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--network-shuffle--network-shuffle-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--sketch--sketch-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--common--unsafe--unsafe-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--core-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_generated_resources.jar:/databricks/jars/----workspace_spark_3_2--core--libcore_resources.jar:/databricks/jars/----workspace_spark_3_2--core--proto-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--graphx--graphx-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--launcher--launcher-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.12.189.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.chuusai--shapeless_2.12--com.chuusai__shapeless_2.12__2.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks--Rserve--com.databricks__Rserve__1.8-3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--kryo-shaded--com.esotericsoftware__kryo-shaded__4.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.esotericsoftware--minlog--com.esotericsoftware__minlog__1.3.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.datatype--jackson-datatype-joda--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-paranamer--com.fasterxml.jackson.module__jackson-module-paranamer__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.12.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil--jniloader--com.github.fommil__jniloader__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--core--com.github.fommil.netlib__core__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_ref-java-natives--com.github.fommil.netlib__native_ref-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--native_system-java-natives--com.github.fommil.netlib__native_system-java-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_ref-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_ref-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.fommil.netlib--netlib-native_system-linux-x86_64-natives--com.github.fommil.netlib__netlib-native_system-linux-x86_64-natives__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.luben--zstd-jni--com.github.luben__zstd-jni__1.5.0-4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.github.wendykierp--JTransforms--com.github.wendykierp__JTransforms__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.code.gson--gson--com.google.code.gson__gson__2.8.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.crypto.tink--tink--com.google.crypto.tink__tink__1.6.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__1.9.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.guava--guava--com.google.guava__guava__15.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.h2database--h2--com.h2database__h2__1.4.195.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.helger--profiler--com.helger__profiler__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jcraft--jsch--com.jcraft__jsch__0.1.50.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.lihaoyi--sourcecode_2.12--com.lihaoyi__sourcecode_2.12__0.1.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.ning--compress-lzf--com.ning__compress-lzf__1.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.istack--istack-commons-runtime--com.sun.istack__istack-commons-runtime__3.0.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.sun.mail--javax.mail--com.sun.mail__javax.mail__1.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill-java--com.twitter__chill-java__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--chill_2.12--com.twitter__chill_2.12__0.10.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe--config--com.typesafe__config__1.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.univocity--univocity-parsers--com.univocity__univocity-parsers__2.9.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-codec--commons-codec--commons-codec__commons-codec__1.15.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-dbcp--commons-dbcp--commons-dbcp__commons-dbcp__1.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-io--commons-io--commons-io__commons-io__2.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-net--commons-net--commons-net__commons-net__3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--arpack--dev.ludovic.netlib__arpack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--blas--dev.ludovic.netlib__blas__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--dev.ludovic.netlib--lapack--dev.ludovic.netlib__lapack__2.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.airlift--aircompressor--io.airlift__aircompressor__0.21.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.delta--delta-sharing-spark_2.12--io.delta__delta-sharing-spark_2.12__0.4.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-graphite--io.dropwizard.metrics__metrics-graphite__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.netty--netty-all--io.netty__netty-all__4.1.68.Final.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_common--io.prometheus__simpleclient_common__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_pushgateway--io.prometheus__simpleclient_pushgateway__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--io.prometheus.jmx--collector--io.prometheus.jmx__collector__0.12.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.annotation--jakarta.annotation-api--jakarta.annotation__jakarta.annotation-api__1.3.5.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.servlet--jakarta.servlet-api--jakarta.servlet__jakarta.servlet-api__4.0.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jakarta.ws.rs--jakarta.ws.rs-api--jakarta.ws.rs__jakarta.ws.rs-api__2.1.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.activation--activation--javax.activation__activation__1.1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.annotation--javax.annotation-api--javax.annotation__javax.annotation-api__1.3.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--jta--javax.transaction__jta__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javax.xml.stream--stax-api--javax.xml.stream__stax-api__1.0-2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jets3t-0.7--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--jline--jline--jline__jline__2.14.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--joda-time--joda-time--joda-time__joda-time__2.10.10.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--liball_deps_2.12.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--log4j--log4j--log4j__log4j__1.2.17.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.razorvine--pyrolite--net.razorvine__pyrolite__4.30.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.opencsv--opencsv--net.sf.opencsv__opencsv__2.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sf.supercsv--super-csv--net.sf.supercsv__super-csv__2.2.0.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-ingest-sdk--net.snowflake__snowflake-ingest-sdk__0.9.6.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--snowflake-jdbc--net.snowflake__snowflake-jdbc__3.13.3.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.9.0-spark_3.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--net.sourceforge.f2j--arpack_combined_all--net.sourceforge.f2j__arpack_combined_all__0.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.5.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--antlr4-runtime--org.antlr__antlr4-runtime__4.8.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:/databricks/jars/----workspace_spark_3_2--maven-trees--hive-2.3__hadoop-3.2--o\n*** WARNING: skipped 86485 bytes of output ***\n\n (&#39;spark.databricks.clusterUsageTags.enableSqlAclsOnly&#39;, &#39;false&#39;),\n (&#39;spark.repl.class.uri&#39;, &#39;spark://10.139.64.4:43533/classes&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterSizeType&#39;, &#39;VM_CONTAINER&#39;),\n (&#39;spark.hadoop.databricks.fs.perfMetrics.enable&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.userProvidedSparkVersion&#39;,\n  &#39;10.4.x-scala2.12&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumSshKeys&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverContainerId&#39;,\n  &#39;931b260a3f104de1bc3034bc164cb253&#39;),\n (&#39;spark.hadoop.fs.gs.outputstream.upload.chunk.size&#39;, &#39;16777216&#39;),\n (&#39;spark.speculation.quantile&#39;, &#39;0.9&#39;),\n (&#39;spark.databricks.clusterUsageTags.privateLinkEnabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.manager&#39;, &#39;SORT&#39;),\n (&#39;spark.files.overwrite&#39;, &#39;true&#39;),\n (&#39;spark.databricks.credential.aws.secretKey.redactor&#39;,\n  &#39;com.databricks.spark.util.AWSSecretKeyRedactorProxy&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterNumCustomTags&#39;, &#39;0&#39;),\n (&#39;spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes&#39;,\n  &#39;false&#39;),\n (&#39;spark.r.numRBackendThreads&#39;, &#39;1&#39;),\n (&#39;spark.hadoop.fs.wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.abfss.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.workspace.multipleResults.enabled&#39;, &#39;true&#39;),\n (&#39;spark.sql.hive.metastore.version&#39;, &#39;0.13.0&#39;),\n (&#39;spark.shuffle.service.port&#39;, &#39;4048&#39;),\n (&#39;spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType&#39;, &#39;default&#39;),\n (&#39;spark.databricks.acl.client&#39;,\n  &#39;com.databricks.spark.sql.acl.client.SparkSqlAclClient&#39;),\n (&#39;spark.streaming.driver.writeAheadLog.closeFileAfterWrite&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.warehouse.subdir.inherit.perms&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.mcfs-abfss.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.ManagedCatalogFileSystem&#39;),\n (&#39;spark.r.sql.derby.temp.dir&#39;, &#39;/tmp/Rtmp2Unrs7&#39;),\n (&#39;spark.hadoop.fs.s3n.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-wasbs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.databricks.passthrough.adls.tokenProviderClassName&#39;,\n  &#39;com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider&#39;),\n (&#39;spark.app.name&#39;, &#39;Databricks Shell&#39;),\n (&#39;spark.driver.allowMultipleContexts&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.AbstractFileSystem.gs.impl&#39;,\n  &#39;shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS&#39;),\n (&#39;spark.databricks.secret.sparkConf.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.rdd.compress&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env&#39;, &#39;&#39;),\n (&#39;spark.databricks.eventLog.dir&#39;, &#39;eventlogs&#39;),\n (&#39;spark.databricks.driverNfs.pathSuffix&#39;, &#39;.ephemeral_nfs&#39;),\n (&#39;spark.app.id&#39;, &#39;app-20220420015418-0000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterCreator&#39;, &#39;Webapp&#39;),\n (&#39;spark.speculation&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.hive.server2.session.check.interval&#39;, &#39;60000&#39;),\n (&#39;spark.sql.hive.convertCTAS&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.fs.s3a.max.total.tasks&#39;, &#39;1000&#39;),\n (&#39;spark.hadoop.spark.sql.parquet.output.committer.class&#39;,\n  &#39;org.apache.spark.sql.parquet.DirectParquetOutputCommitter&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverPublicDns&#39;, &#39;104.42.101.153&#39;),\n (&#39;spark.databricks.tahoe.logStore.aws.class&#39;,\n  &#39;com.databricks.tahoe.store.MultiClusterLogStore&#39;),\n (&#39;spark.hadoop.fs.s3a.fast.upload.default&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterGeneration&#39;, &#39;0&#39;),\n (&#39;spark.hadoop.fs.abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.speculation.multiplier&#39;, &#39;3&#39;),\n (&#39;spark.storage.blockManagerTimeoutIntervalMs&#39;, &#39;300000&#39;),\n (&#39;spark.sparkr.use.daemon&#39;, &#39;false&#39;),\n (&#39;spark.scheduler.listenerbus.eventqueue.capacity&#39;, &#39;20000&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterStateMessage&#39;, &#39;Starting Spark&#39;),\n (&#39;spark.hadoop.parquet.page.write-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.clusterName&#39;, &#39;democluster&#39;),\n (&#39;spark.hadoop.databricks.s3commit.client.sslTrustAll&#39;, &#39;false&#39;),\n (&#39;spark.hadoop.fs.s3a.threads.max&#39;, &#39;136&#39;),\n (&#39;spark.r.backendConnectionTimeout&#39;, &#39;604800&#39;),\n (&#39;spark.hadoop.hive.server2.idle.session.timeout&#39;, &#39;900000&#39;),\n (&#39;spark.databricks.redactor&#39;,\n  &#39;com.databricks.spark.util.DatabricksSparkLogRedactorProxy&#39;),\n (&#39;spark.hadoop.fs.s3a.impl&#39;,\n  &#39;shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem&#39;),\n (&#39;spark.hadoop.fs.fcfs-abfs.impl.disable.cache&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.parquet.page.verify-checksum.enabled&#39;, &#39;true&#39;),\n (&#39;spark.logConf&#39;, &#39;true&#39;),\n (&#39;spark.databricks.clusterUsageTags.dataPlaneRegion&#39;, &#39;westus&#39;),\n (&#39;spark.databricks.clusterUsageTags.enableJobsAutostart&#39;, &#39;true&#39;),\n (&#39;spark.hadoop.hive.server2.enable.doAs&#39;, &#39;false&#39;),\n (&#39;eventLog.rolloverIntervalSeconds&#39;, &#39;3600&#39;),\n (&#39;spark.databricks.clusterUsageTags.driverInstanceId&#39;,\n  &#39;fa8156c19b3f40be98a67631de656e7f&#39;),\n (&#39;spark.hadoop.parquet.filter.columnindex.enabled&#39;, &#39;false&#39;),\n (&#39;spark.shuffle.memoryFraction&#39;, &#39;0.2&#39;),\n (&#39;spark.driver.port&#39;, &#39;43533&#39;),\n (&#39;spark.hadoop.fs.dbfsartifacts.impl&#39;,\n  &#39;com.databricks.backend.daemon.data.client.DBFSV1&#39;),\n (&#39;spark.hadoop.fs.cpfs-s3a.impl&#39;,\n  &#39;com.databricks.sql.acl.fs.CredentialPassthroughFileSystem&#39;),\n (&#39;spark.hadoop.fs.s3a.connection.timeout&#39;, &#39;50000&#39;),\n (&#39;spark.databricks.secret.envVar.keys.toRedact&#39;, &#39;&#39;),\n (&#39;spark.databricks.clusterUsageTags.cloudProvider&#39;, &#39;Azure&#39;),\n (&#39;spark.files.useFetchCache&#39;, &#39;false&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90d4a49d-aad2-41ad-a671-0ee556b5ca2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8332887818640824#setting/sparkui/0420-014945-374ax7z1/driver-6463660330994956308\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8332887818640824#setting/sparkui/0420-014945-374ax7z1/driver-6463660330994956308\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, let's create our first dataframe from a fairly small sample data set. We have been working with a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, seeing an ad. \n\nHere, we will read in just one of the json files:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71edb37b-2799-4806-83c8-fd6b2c85f0e2"}}},{"cell_type":"code","source":["path = \"/FileStore/sparkify_log_small.json\"\nuser_log = spark.read.json(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc642add-ff5b-4947-a429-e44723740564"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e94aff1-8275-4483-81dd-25ca85ba3732"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7f89cbd-34e5-4a6d-a28e-1961fae9113b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[27]: DataFrame[summary: string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: DataFrame[summary: string]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.show(n=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"132fc363-36fa-4cd9-9dcd-be25f511157c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">++\n||\n++\n++\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">++\n|\n++\n++\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa93cec-f6e7-42c8-9305-00fa5cfbd2e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[29]: []</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: []</div>"]}}],"execution_count":0},{"cell_type":"code","source":["out_path = \"/delta/sparkify_log_small.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fddba62-0253-4941-bceb-d58c9b02de5e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log.write.format(\"delta\").mode(\"overwrite\").save(out_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2788cead-c033-4cfd-80ec-b8e6ef412784"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2 = spark.read.format(\"delta\").load(out_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77b0dc41-1e9b-462d-a6ae-d0e1e13a9f58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32fc40c3-de59-4f27-9bf8-4a2cb9cc5431"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: double (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- artist: string (nullable = true)\n-- auth: string (nullable = true)\n-- firstName: string (nullable = true)\n-- gender: string (nullable = true)\n-- itemInSession: long (nullable = true)\n-- lastName: string (nullable = true)\n-- length: double (nullable = true)\n-- level: string (nullable = true)\n-- location: string (nullable = true)\n-- method: string (nullable = true)\n-- page: string (nullable = true)\n-- registration: double (nullable = true)\n-- sessionId: long (nullable = true)\n-- song: string (nullable = true)\n-- status: long (nullable = true)\n-- ts: long (nullable = true)\n-- userAgent: string (nullable = true)\n-- userId: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc9eaa00-c540-4bea-91e4-21dbf60bd0ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[34]: []</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: []</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.select(\"userID\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cc1f7eb-cb2d-46e5-966e-ec071c5df54c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+\n|userID|\n+------+\n+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nuserID|\n+------+\n+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["user_log_2.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6651c6a1-c343-4874-a913-22402542a0b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: []</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: []</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(user_log_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9c1fe9b-5ba4-43ce-812b-1d05c5ca2215"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"artist","type":"\"string\"","metadata":"{}"},{"name":"auth","type":"\"string\"","metadata":"{}"},{"name":"firstName","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"itemInSession","type":"\"long\"","metadata":"{}"},{"name":"lastName","type":"\"string\"","metadata":"{}"},{"name":"length","type":"\"double\"","metadata":"{}"},{"name":"level","type":"\"string\"","metadata":"{}"},{"name":"location","type":"\"string\"","metadata":"{}"},{"name":"method","type":"\"string\"","metadata":"{}"},{"name":"page","type":"\"string\"","metadata":"{}"},{"name":"registration","type":"\"double\"","metadata":"{}"},{"name":"sessionId","type":"\"long\"","metadata":"{}"},{"name":"song","type":"\"string\"","metadata":"{}"},{"name":"status","type":"\"long\"","metadata":"{}"},{"name":"ts","type":"\"long\"","metadata":"{}"},{"name":"userAgent","type":"\"string\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>artist</th><th>auth</th><th>firstName</th><th>gender</th><th>itemInSession</th><th>lastName</th><th>length</th><th>level</th><th>location</th><th>method</th><th>page</th><th>registration</th><th>sessionId</th><th>song</th><th>status</th><th>ts</th><th>userAgent</th><th>userId</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae6a6d76-9f02-49d0-8918-cb08ee7de4ae"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"3_data_inputs_and_outputs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":676075729546765}},"nbformat":4,"nbformat_minor":0}

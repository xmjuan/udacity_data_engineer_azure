## Project Introduction:
This is data modeling project in Udacity data engineer nano degree in microsoft azure. In this project, I am asked to build data modeling and pipelines with postgres in star schema for a startup called Sparkify so that they can analyze what songs the users've been listening to.

## Datasets:
- Original:
>> Log data: log files in JSON format generated by event simulator. This dataset is the activity logs from a music streaming app. 
>> Song data: song files in JSON format genereted from Million Song Dataset. This dataset is the song profile including artist and song info.

- Ingested:
>> Fact Table:
>>> songplays - records in log data associated with song plays i.e. records ith page NextSong songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
>> Dimension Tables:
>>> users - users in the app user_id, first_name, last_name, gender, level
>>> songs - songs in music database song_id, title, artist_id, year, duration >>> artists - artists in music database artist_id, name, location, latitude, longitude
>>> time - timestamps of records in songplays broken down into specific units start_time, hour, day, week, month, year, weekday

## Files explaination & how to run:
-- sql_queries.py: this file stores all sql queries of dropping/creating/inserting/finding tables/records.

-- create_tables.py: this file is used to build connector, create database. (To run etl.py, this file needs to be run first.)

-- etl.ipynb: this file is a preliminary step before creating etl.py. In this file, etl process is broken down in multiple steps in jupyter notebook so that debugging is easier.

-- etl.py -> consolidated etl steps are in this file, this file produces the all fact and dimension tables.

-- test.ipynb -> this file is to test and validate if the created database and tables are qualified.

-- To run the pipelines, follow below steps:
!python create_tables.py
!python etl.py

## Statement:
Due to the analysis goal of Sparkify, I produced tables in star schema to facilate easier aggregation.
In the ETL process, all necessary conditions are defined for each table. However, for all tables I used 'ON CONFLICT DO NOTHING' statement which assumes the the info entered for the first time is correct. This might need to be adjusted for reality to be 'ON CONFLICT DO UPDATE' thinking about data might be corrected later.

## Reference:
https://www.postgresqltutorial.com/postgresql-upsert/
https://www.w3schools.com/sql/
https://stackoverflow.com/questions/44287011/valueerror-expected-object-or-value-when-reading-json-as-pandas-dataframe